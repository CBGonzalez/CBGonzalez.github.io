---
layout: post
title: It´s official – AI systems are like kids
excerpt: Five AI research areas that look like they came from a parent.
date: 2018-06-08
categories: [AI, society]
---
Have a look at this list of 5 research areas related to accident risk (the examples are for a simple cleaning robot):
1. **Avoid Negative side effects** (e.g. knocking over a vase while trying to move a box from A to B efficiently)
2. **Avoid Reward hacking** (e.g. creating work for itself in order to achieve more “points”)
3. **Provide Scalable oversight** (i. e. how to avoid constant human control to avoid accidents)
4. **Permit Safe exploration** (i. e. how to permit autonomous but safe exploring)
5. **Evaluate Robustness** to distributional shift (i.e. how to transition from a learning environment to the real world)

Parents will easily recognize many or all of these. Since the current AI paradigm is an explicit mimicry of human intelligence and its capabilities that is no accident.
The only ways we know of to produce intelligence and skills in adult humans are genetics (if we assume that “intelligence” is at least partly genetic, “built-in rules”) and education (“nurture”, upbringing, training or whatever we might call it) with its AI analogous in machine learning (ML), specifically what is called [reinforcement learning (RL)](https://en.wikipedia.org/wiki/Reinforcement_learning) or [supervised learning](https://en.wikipedia.org/wiki/Supervised_learning).
(The list above comes from a [Google Research](http://research.google.com/pubs/papers.html) paper ([Concrete Problems in AI safety](https://arxiv.org/pdf/1606.06565.pdf), Dario Amodei, Chris Olah et al) in which the authors use the example of a hypothetical cleaning robot to delve into the items shown).


> A common joke (by teachers mostly) is that we know a lot about teaching… but so little about learning. If we had full confidence and understanding of learning (as we have, say, of manufacturing processes) life would be very different. Imagine a driving course with learning V2.0: no driving test would be necessary since we are sure that all that was taught is also learned. All we would need was regular sampling of a small subset of new drivers just to make sure all was well and to be able to tweak the lessons. Same for physics, medicine, anything really.

>Instead a few thousand years of civilization have left us with more or less complicated ways to be reasonably sure that e. g. doctors know everything they need to safely treat us, or that airline pilots will not fly us into danger. This involves specific teachings techniques (e. g. in medicine memorization of hundreds of bones), selection procedures (30 hour shifts for interns to weed out the stress-shy doctors) and much more. And of course, regular exams and tests. Not to forget a valid diploma or license as pinnacle of the whole process and the possibility to punish activities that prove to be unsafe.

> All of this probably because we really are not able to exhaustively list all the “knowledge [*]” required to be a good doctor or pilot. And even more difficult, we are not able to precisely define the “common sense” (i.e. the underlying common knowledge and values) that will be the basis of all learning. We rely instead on a lengthy childhood and adolescence in school to gather commonly accepted values, morals, “knowledge” and habits. And societies are very conservative when it comes to changing any of this.

> Or, more to the point, we are trying to mimic learning, and it is something we poorly understand.


Back to AI and ML: the choice of the current paradigms has consequences that will lead to inevitable regulation (by governments or industries, driven by public clamor or huge lawsuits) of certification procedures before “distributional shift” can happen. All it will take is a few catastrophic failures involving large property damage or human loss… The Tesla autopilot feature might just be the first trigger (to be fair though, mostly through human stupidity).

One of the problems of human learning seems to be the seemingly infinite diversity of personality traits that might affect it. If we got with that for a moment, one could argue that AI does not have this problem: one company will produce copies of, say, one cleaning AI that went through cleaner´s training.

But the diversity problem in human learning is not only one of varying personalities. The experience everybody goes through while learning and later working will be unique. So one could just freeze the AI learning once the robot is shipped … which would probably defeat the whole purpose of it since you would have just a (sophisticated) dumb robot.

Human societies try to tackle this problem by standardizing the learning and the (initial) experience gathering through schools and universities. Standards for RL and supervised learning will be imposed to the AI industry, if not by governments then by lawyers. And an AI driver´s test might just be around the corner.


[*] [Knowledge](https://en.wikipedia.org/wiki/Knowledge) is a difficult and loaded concept. It is used here loosely as a summation of skills learned and information memorized.

